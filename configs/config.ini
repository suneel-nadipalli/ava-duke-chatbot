[BITS_AND_BYTES]
load_in_4bit = True
bnb_4bit_quant_type = "nf4"
bnb_4bit_use_double_quant = True
bnb_4bit_compute_dtype = torch.bfloat16

[LORA_CONFIG]
r = 16
lora_alpha = 32
lora_dropout = 0.05
bias = 'none'
task_type = 'CAUSAL_LM'

[GEN_CONFIG]
max_new_tokens = 200
temperature = 0.7
top_p = 0.7
num_return_sequences = 1

[TRAINING_ARGS]
per_device_train_batch_size = 1
gradient_accumulation_steps = 4
num_train_epochs = 1
learning_rate = 2e-4
fp16 = True
save_total_limit = 3
logging_steps = 1
output_dir = "data/output/experiments"
max_steps = 5
optim = "paged_adamw_8bit"
lr_scheduler_type = "cosine"
warmup_ratio = 0.05

[SECRETS]

hf_token = ""
api_url = "https://z42ydp6bpbh9juql.us-east-1.aws.endpoints.huggingface.cloud"
