{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import certifi\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import openai\n",
    "\n",
    "# Import the OpenAI API key\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Chunks to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database():\n",
    "    \"\"\"\n",
    "    Purpose: Establish a connection to the MongoDB database.\n",
    "    \"\"\"\n",
    "    uri = \"mongodb+srv://sriveerisetti:8TkNOyysCO4S3lBo@chatbot.w3bjnk6.mongodb.net/?retryWrites=true&w=majority&appName=Chatbot\"\n",
    "    ca = certifi.where()\n",
    "    # The MongoClient is used to establish a connection to the database.\n",
    "    client = MongoClient(uri, tlsCAFile=ca)\n",
    "    # The database that contains the information is Chatbot\n",
    "    db = client['Chatbot'] \n",
    "    return db\n",
    "\n",
    "def embed_message(user_message):\n",
    "    \"\"\"\n",
    "    Purpose: Embed the user's message using the GIST-large model.\n",
    "    Input: user_message - The message that the user has entered.\n",
    "    \"\"\"\n",
    "    # We ended up using Hugging Face's SentenceTransformer library to embed the user's message.\n",
    "    # The GIST embedding model is on the leaderboard on Hugging Face\n",
    "    model = SentenceTransformer(\"avsolatorio/GIST-large-Embedding-v0\")\n",
    "    # We use the encode functoin to embed the user's message\n",
    "    query_embedding = model.encode([user_message], convert_to_tensor=True).tolist()[0]\n",
    "    return query_embedding\n",
    "\n",
    "def chunk_words(text, chunk_size=150, overlap=25):\n",
    "    \"\"\"\n",
    "    Purpose: Split the text into chunks of a specified size with a specified overlap.\n",
    "    Input: text - The text to split into chunks.\n",
    "    Input: chunk_size - The size of each chunk (150).\n",
    "    Input: overlap - The number of words to overlap between chunks (25).\n",
    "    \"\"\"\n",
    "    # The purpose of this code is to make sure that the chunk size remains firmly at 150 words.\n",
    "    # We use 25 words from the previous chunk to overlap with the next chunk and then 25 words from the next chunk to overlap with the previous chunk.\n",
    "    # In total there are 150 words in each chunk.\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0 \n",
    "    # Here we are making sure that the chunk size is 150 words.\n",
    "    while start + chunk_size - 2 * overlap < len(words):\n",
    "        # We are making sure that the start is at 0.\n",
    "        if start == 0:\n",
    "            actual_start = start\n",
    "            # We are making sure that the end is at 150 words.\n",
    "            actual_end = start + chunk_size - overlap  \n",
    "        else:\n",
    "            actual_start = start - overlap\n",
    "            actual_end = start + chunk_size - overlap\n",
    "        # Here we make sure tha the end is not greater than the length of the words (150)\n",
    "        if actual_end + overlap > len(words):\n",
    "            actual_end = len(words)  \n",
    "        # Here we gather the words in the chunk.\n",
    "        chunk = words[actual_start:actual_end]\n",
    "        # Here we append the words in the chunk to the chunks list.\n",
    "        chunks.append(' '.join(chunk))\n",
    "        start += chunk_size - 2 * overlap\n",
    "    # Here we are making sure that the last chunk is not greater than the length of the words.\n",
    "    if start < len(words):\n",
    "        # We use the max function to make sure of this.\n",
    "        last_chunk_start = max(0, start - overlap)\n",
    "        last_chunk = words[last_chunk_start:len(words)]\n",
    "        chunks.append(' '.join(last_chunk))\n",
    "    return chunks\n",
    "\n",
    "def store_text_with_embedding(text, source, collection):\n",
    "    \"\"\"\n",
    "    Store the text and its embedding in the database.\n",
    "    :param text: The text to store\n",
    "    :param source: The source of the text\n",
    "    :param collection: The MongoDB collection in which to store the text\n",
    "    \"\"\"\n",
    "    for chunk in chunk_words(text):\n",
    "        chunk_embedding = embed_message(chunk)  # Use BERT-based embedding\n",
    "        collection.insert_one({\n",
    "            \"chunk\": chunk,\n",
    "            \"embedding\": chunk_embedding,\n",
    "            \"source\": source\n",
    "        })\n",
    "    print(f\"Content from {source} has been successfully stored in MongoDB.\")\n",
    "\n",
    "def process_text_files(folder_path, collection):\n",
    "    \"\"\"\n",
    "    Purpose: Process the text files in the specified folder and store the text and its embedding in the database.\n",
    "    Input: folder_path - The path to the folder containing the text files.\n",
    "    Input: collection - The MongoDB collection in which to store the text.\n",
    "    \"\"\"\n",
    "    # We create a for loop that goes through the folder containing the text files.\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # For all files within the folder we read them and use the store_text_with_embedding function to store the text and its \n",
    "            # embedding in the database.\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text_content = file.read()\n",
    "                store_text_with_embedding(text_content, filename, collection)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db = get_database()\n",
    "    collection = db['Duke5']\n",
    "    folder_path = \"/content/Combo_Data\"\n",
    "    process_text_files(folder_path, collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Relevant Chunks and Try With GPT 3.5 (Just to see if it is working properly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database():\n",
    "    \"\"\"\n",
    "    Purpose: Establish a connection to the MongoDB database.\n",
    "    \"\"\"\n",
    "    uri = \"mongodb+srv://sriveerisetti:8TkNOyysCO4S3lBo@chatbot.w3bjnk6.mongodb.net/?retryWrites=true&w=majority&appName=Chatbot\"\n",
    "    ca = certifi.where()\n",
    "    # The MongoClient is used to establish a connection to the database.\n",
    "    client = MongoClient(uri, tlsCAFile=ca)\n",
    "    # The database that contains the information is Chatbot\n",
    "    db = client['Chatbot'] \n",
    "    return db\n",
    "\n",
    "def generate_embedding(user_message):\n",
    "    \"\"\"\n",
    "    Purpose: Embed the user's message using the GIST-large model.\n",
    "    Input: user_message - The message that the user has entered.\n",
    "    \"\"\"\n",
    "    # We ended up using Hugging Face's SentenceTransformer library to embed the user's message.\n",
    "    # The GIST embedding model is on the leaderboard on Hugging Face\n",
    "    model = SentenceTransformer(\"avsolatorio/GIST-large-Embedding-v0\")\n",
    "    # We use the encode functoin to embed the user's message\n",
    "    query_embedding = model.encode([user_message], convert_to_tensor=True).tolist()[0]\n",
    "    return query_embedding\n",
    "\n",
    "def find_most_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Purpose: Find the most relevant chunks to the user's query.\n",
    "    Input: query - the user's query\n",
    "    Input: top_k - the number of most relevant chunks to return\n",
    "    \"\"\"\n",
    "    # Here we connect to the database and also define the collection within the database \n",
    "    db = get_database()\n",
    "    collection = db['Duke5']\n",
    "    # We use the generate_embedding function to generate the embedding for the user's query\n",
    "    query_embedding = np.array(generate_embedding(query)).reshape(1, -1)  \n",
    "    docs = collection.find({})\n",
    "\n",
    "    # Empty list to store the similarities\n",
    "    similarities = []\n",
    "    # For all chunks in the collection we calculate the cosine similarity between the query embedding and the document embedding\n",
    "    for doc in docs:\n",
    "        chunk_embedding = np.array(doc['embedding']).reshape(1, -1)  \n",
    "        # We use the cosine similarity function to calculate the similarity between the query embedding and the document embedding\n",
    "        similarity = cosine_similarity(chunk_embedding, query_embedding)[0][0]\n",
    "        # We append the chunk, similarity and source to the similarities list\n",
    "        similarities.append((doc['chunk'], similarity, doc.get('source')))\n",
    "\n",
    "    # We sort the similarities list in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    seen_chunks = set()\n",
    "    unique_similarities = []\n",
    "    # We iterate over the similarities list and add the most relevant chunks to the unique_similarities list\n",
    "    for chunk, similarity, source in similarities:\n",
    "        if chunk not in seen_chunks:\n",
    "            # If the chunk is unique, then we add it to the unique_similarities list\n",
    "            seen_chunks.add(chunk)\n",
    "            unique_similarities.append((chunk, similarity, source))\n",
    "            if len(unique_similarities) == top_k:\n",
    "                break\n",
    "    return unique_similarities\n",
    "\n",
    "def generate_prompt_with_context(relevant_chunks, query):\n",
    "    \"\"\"\n",
    "    Purpose: Generate a prompt that includes the context of the most relevant chunks and the user's query.\n",
    "    Input: relevant_chunks - the most relevant chunks to the user's query\n",
    "    Input: query - the user's query\n",
    "    \"\"\"\n",
    "    # Here, we are creating a context that includes the most relevant chunks to the user's query\n",
    "    context = \"Based on the following information: \"\n",
    "    # Here we iterate over the relevant chunks and add them to the context\n",
    "    for chunk, similarity, source in relevant_chunks:\n",
    "        context += f\"\\n- [Source: {source}]: {chunk}\"\n",
    "    # Here we add the user's query to the prompt \n",
    "    prompt = f\"{context}\\n\\n{query}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_text_with_gpt35(prompt, max_tokens=3100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Purpose: Generate text using the GPT-3.5 model with the specified prompt.\n",
    "    Input: prompt - the prompt for the model\n",
    "    Input: max_tokens - the maximum number of tokens to generate\n",
    "    Input: temperature - controls the randomness of the output, higher values lead to more varied outputs\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        # Here we use the GPT-3.5-turbo model to generate the text\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            # We make sure to set the role of the system as an expert on the Duke Artificial Intelligence Master of Engineering Program\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on the Duke Artificial Intelligence Master of Engineering Program\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature, \n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def get_response_for_query(query, temperature=0.9):\n",
    "    \"\"\"\n",
    "    Purpose: Get a response for the user's query.\n",
    "    Input: query - the user's query\n",
    "    Input: temperature - controls the randomness of the output, higher values lead to more varied outputs\n",
    "    \"\"\"\n",
    "    # We use the find_most_relevant_chunks function to find the most relevant chunks to the user's query\n",
    "    relevant_chunks = find_most_relevant_chunks(query)\n",
    "    if relevant_chunks:\n",
    "        # We use the generate_prompt_with_context function to generate a prompt that includes the context of the most relevant chunks and the user's query\n",
    "        prompt = generate_prompt_with_context(relevant_chunks, query)\n",
    "    else:\n",
    "        prompt = query \n",
    "    # We use the generate_text_with_gpt35 function to generate text using the GPT-3.5 model with the specified prompt\n",
    "    return generate_text_with_gpt35(prompt, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the Dataset Samples: Take Curated Questions from GPT 3.5 and increase size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_questions_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Purpose: Read questions from a CSV file.\n",
    "    Input: file_path - the path to the CSV file containing the questions.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    # Here we loop through the text file and read the questions\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # For each row within the question, we append the question to the questions list\n",
    "        for row in reader:\n",
    "            # Append the question to the questions list\n",
    "            questions.append(row[0])\n",
    "    return questions\n",
    "\n",
    "def rephrase_questions(questions, target_total, variations_per_question=2): \n",
    "    \"\"\"\n",
    "    Purpose: Rephrase the questions using the GPT-3.5 model.\n",
    "    Input: questions - the list of questions to rephrase\n",
    "    Input: target_total - the target total number of questions to generate\n",
    "    Input: variations_per_question - the number of variations to generate per question\n",
    "    \"\"\" \n",
    "    rephrased_questions = []\n",
    "    # Here we calculate the number of questions needed to reach the target total\n",
    "    # There are roughly 328 questions in the original questions list\n",
    "    needed = max(target_total - len(questions), 0)\n",
    "\n",
    "    for question in questions:\n",
    "        # Default count variable to keep track of the number of rephrased questions\n",
    "        count = 0\n",
    "        # While the count is less than the variations per question and the length of the rephrased questions is less than the needed questions\n",
    "        # we generate different ways to ask the question\n",
    "        while count < variations_per_question and len(rephrased_questions) < needed:\n",
    "            # This prompt is used to generate different ways to ask the question\n",
    "            prompt_text = f\"Generate {variations_per_question} different ways to ask the following question: {question}\"\n",
    "            try:\n",
    "                response = openai.Completion.create(\n",
    "                    # We use the GPT-3.5-turbo-instruct model to generate the rephrased questions\n",
    "                    engine=\"gpt-3.5-turbo-instruct\",\n",
    "                    prompt=prompt_text,\n",
    "                    max_tokens=100,\n",
    "                    n=variations_per_question,\n",
    "                    stop=None,\n",
    "                    temperature=0.8\n",
    "                )\n",
    "                # For each rephrased question in the response, we append it to the rephrased questions list\n",
    "                for rephrased in response.choices[0].text.strip().split('\\n'):\n",
    "                    # If the rephrased question is not empty and is not the same as the original question, we append it to the rephrased questions list\n",
    "                    if rephrased.strip() and rephrased.strip() != question:\n",
    "                        rephrased_questions.append(rephrased.strip())\n",
    "                        # We increase the count variable tracker by 1 each time we add a rephrased question\n",
    "                        count += 1\n",
    "                        if len(rephrased_questions) >= needed:\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question: {str(e)}\")\n",
    "\n",
    "    return questions + rephrased_questions \n",
    "\n",
    "def save_questions_to_csv(questions, output_path):\n",
    "    \"\"\"\n",
    "    Purpose: Save the questions to a CSV file.\n",
    "    Input: questions - the list of questions to save\n",
    "    Input: output_path - the path to the CSV file to save the questions\n",
    "    \"\"\"\n",
    "    # We open a new CSV file and write the questions to the file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        # We use the csv writer to write the questions to the file\n",
    "        writer = csv.writer(csvfile)\n",
    "        # For all questions in the questions list, we write the question to the file\n",
    "        for question in questions:\n",
    "            writer.writerow([question])\n",
    "\n",
    "# Paths for the input and output files\n",
    "input_file_path = '/content/questions.csv'  \n",
    "output_file_path = '/content/rephrased_questions.csv'  \n",
    "\n",
    "# Call functions to rephrase the questions\n",
    "original_questions = read_questions_from_csv(input_file_path)\n",
    "# The target total is set to 700\n",
    "rephrased_questions = rephrase_questions(original_questions, 700)  \n",
    "# Save the rephrased questions to a CSV file\n",
    "save_questions_to_csv(rephrased_questions, output_file_path)\n",
    "\n",
    "# Track the number of questions generated\n",
    "print(f\"Total questions generated: {len(rephrased_questions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Question/Answer Pairs for Training using GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database():\n",
    "    \"\"\"\n",
    "    Purpose: Establish a connection to the MongoDB database.\n",
    "    \"\"\"\n",
    "    uri = \"mongodb+srv://sriveerisetti:8TkNOyysCO4S3lBo@chatbot.w3bjnk6.mongodb.net/?retryWrites=true&w=majority&appName=Chatbot\"\n",
    "    ca = certifi.where()\n",
    "    # The MongoClient is used to establish a connection to the database.\n",
    "    client = MongoClient(uri, tlsCAFile=ca)\n",
    "    # The database that contains the information is Chatbot\n",
    "    db = client['Chatbot'] \n",
    "    return db\n",
    "\n",
    "def generate_embedding(user_message):\n",
    "    \"\"\"\n",
    "    Purpose: Embed the user's message using the GIST-large model.\n",
    "    Input: user_message - The message that the user has entered.\n",
    "    \"\"\"\n",
    "    # We ended up using Hugging Face's SentenceTransformer library to embed the user's message.\n",
    "    # The GIST embedding model is on the leaderboard on Hugging Face\n",
    "    model = SentenceTransformer(\"avsolatorio/GIST-large-Embedding-v0\")\n",
    "    # We use the encode functoin to embed the user's message\n",
    "    query_embedding = model.encode([user_message], convert_to_tensor=True).tolist()[0]\n",
    "    return query_embedding\n",
    "\n",
    "def find_most_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Purpose: Find the most relevant chunks to the user's query.\n",
    "    Input: query - the user's query\n",
    "    Input: top_k - the number of most relevant chunks to return\n",
    "    \"\"\"\n",
    "    # Here we connect to the database and also define the collection within the database \n",
    "    db = get_database()\n",
    "    collection = db['Duke5']\n",
    "    # We use the generate_embedding function to generate the embedding for the user's query\n",
    "    query_embedding = np.array(generate_embedding(query)).reshape(1, -1)  \n",
    "    docs = collection.find({})\n",
    "\n",
    "    # Empty list to store the similarities\n",
    "    similarities = []\n",
    "    # For all chunks in the collection we calculate the cosine similarity between the query embedding and the document embedding\n",
    "    for doc in docs:\n",
    "        chunk_embedding = np.array(doc['embedding']).reshape(1, -1)  \n",
    "        # We use the cosine similarity function to calculate the similarity between the query embedding and the document embedding\n",
    "        similarity = cosine_similarity(chunk_embedding, query_embedding)[0][0]\n",
    "        # We append the chunk, similarity and source to the similarities list\n",
    "        similarities.append((doc['chunk'], similarity, doc.get('source')))\n",
    "\n",
    "    # We sort the similarities list in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    seen_chunks = set()\n",
    "    unique_similarities = []\n",
    "    # We iterate over the similarities list and add the most relevant chunks to the unique_similarities list\n",
    "    for chunk, similarity, source in similarities:\n",
    "        if chunk not in seen_chunks:\n",
    "            # If the chunk is unique, then we add it to the unique_similarities list\n",
    "            seen_chunks.add(chunk)\n",
    "            unique_similarities.append((chunk, similarity, source))\n",
    "            if len(unique_similarities) == top_k:\n",
    "                break\n",
    "    return unique_similarities\n",
    "\n",
    "def generate_prompt_with_context(relevant_chunks, query):\n",
    "    \"\"\"\n",
    "    Purpose: Generate a prompt that includes the context of the most relevant chunks and the user's query.\n",
    "    Input: relevant_chunks - the most relevant chunks to the user's query\n",
    "    Input: query - the user's query\n",
    "    \"\"\"\n",
    "    # Here, we are creating a context that includes the most relevant chunks to the user's query\n",
    "    context = \"Based on the following information: \"\n",
    "    # Here we iterate over the relevant chunks and add them to the context\n",
    "    for chunk, similarity, source in relevant_chunks:\n",
    "        context += f\"\\n- [Source: {source}]: {chunk}\"\n",
    "    # Here we add the user's query to the prompt \n",
    "    prompt = f\"{context}\\n\\n{query}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_text_with_gpt35(prompt, max_tokens=3100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Purpose: Generate text using the GPT-3.5 model with the specified prompt.\n",
    "    Input: prompt - the prompt for the model\n",
    "    Input: max_tokens - the maximum number of tokens to generate\n",
    "    Input: temperature - controls the randomness of the output, higher values lead to more varied outputs\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        # Here we use the GPT-3.5-turbo model to generate the text\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            # We make sure to set the role of the system as an expert on the Duke Artificial Intelligence Master of Engineering Program\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on the Duke Artificial Intelligence Master of Engineering Program\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature, \n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def process_questions(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Purpose: Process the questions in the input file and write the answers to the output file.\n",
    "    Input: input_file - the path to the input file containing the questions.\n",
    "    Input: output_file - the path to the output file to write the answers.\n",
    "    \"\"\"\n",
    "    # Here we open a new CSV file to write the answers to the questions\n",
    "    with open(input_file, newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        # We use the csv reader to read the questions from the input file\n",
    "        reader = csv.DictReader(infile)\n",
    "        # We use the csv writer to write the answers to the output file\n",
    "        # We define the field names for the CSV file\n",
    "        writer = csv.DictWriter(outfile, fieldnames=['Question', 'Context', 'Answer'])\n",
    "        writer.writeheader()\n",
    "\n",
    "        # For each row in the input file, we process the question and write the answer to the output file\n",
    "        for row in reader:\n",
    "            # Here we clean the question by getting rid of the numbering\n",
    "            clean_question = re.sub(r'^\\d+\\.\\s+', '', row['Question'])\n",
    "            # Here we print the question that is being processed to make sure that the code is running\n",
    "            print(f\"Processing question: '{clean_question}'\")\n",
    "            # Here we use the find_most_relevant_chunks function to find the most relevant chunks to the user's query\n",
    "            relevant_chunks = find_most_relevant_chunks(clean_question)\n",
    "            # If there are relevant chunks, we generate a prompt with the context of the most relevant chunks and the user's query\n",
    "            if relevant_chunks:\n",
    "                context = \"\\n\".join(f\"{chunk}\" for chunk, _, source in relevant_chunks)  # also removed source prefix here\n",
    "            else:\n",
    "                context = \"No relevant context found.\"\n",
    "            # We use the generate_prompt_with_context function to generate a prompt that includes the context of the most relevant chunks and the user's query\n",
    "            prompt = generate_prompt_with_context(relevant_chunks, clean_question)\n",
    "            # We use the generate_text_with_gpt35 function to generate text using the GPT-3.5 model with the specified prompt\n",
    "            answer = generate_text_with_gpt35(prompt)\n",
    "            # We write the question, context, and answer to the output file\n",
    "            writer.writerow({'Question': clean_question, 'Context': context, 'Answer': answer})\n",
    "            # We print the answer to the question to make sure that the code is running\n",
    "            print(\"Finished processing and writing to CSV.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_questions('/content/rephrased_questions.csv', '/content/qaduke5.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
